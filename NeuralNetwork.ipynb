{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e96370-5197-45fc-ac58-27da38284a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "import cProfile, pstats\n",
    "\n",
    "from QuantumChessGame import * \n",
    "from ChessPuzzles import *\n",
    "from GameToTensor import *\n",
    "from ChessPuzzles import chess_puzzles\n",
    "\n",
    "from MCTS import MCTS_Node\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import QChessNN\n",
    "import MCTS_NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecbdf535-9813-4617-8f75-e383008965ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Declare a new model\n",
    "#NNmodel = QChessNN.QChessNN()\n",
    "\n",
    "\n",
    "\n",
    "# Load the model\n",
    "#NNmodel = QChessNN.QChessNN()\n",
    "NNmodel = torch.jit.load('Puzzle34.pt')\n",
    "NNmodel.eval()\n",
    "\n",
    "game = QuantumChessGame()\n",
    "game.new_game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8c4fe3e-cab3-4570-83a7-8ed2d695c9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#import mathplotlib.pyplot as plt\n",
    "#%mathplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f8d326e-8363-4450-9d14-6f957efb08cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "game = QuantumChessGame()\n",
    "game.new_game()\n",
    "\n",
    "gameData = game.get_game_data()\n",
    "game_tensor = torch.zeros(1,12,8,8)\n",
    "\n",
    "game_tensor[0] = gameToTensor(gameData, 0)\n",
    "#print(game_tensor)\n",
    "y = torch.zeros(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbe234e6-fdb4-4ee4-a159-6be73e6c9070",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTS_AI:\n",
    "    def __init__(self):\n",
    "        return\n",
    "\n",
    "    def find_best_move(self, game, simVar):\n",
    "        root = MCTS_Node(game)\n",
    "        gamedata = game.get_game_data()\n",
    "        bestmove = root.best_action(gamedata.ply, simVar)\n",
    "        return bestmove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51c2a90a-da4d-407d-846d-e1ce8afc1e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkMCTS():\n",
    "    def __init__(self):\n",
    "            return\n",
    "\n",
    "    \n",
    "    def find_best_move(self, game, model, simVar):\n",
    "        root = MCTS_NN.MCTS_Node(game, model)\n",
    "        gamedata = game.get_game_data()\n",
    "        bestmove, value = root.best_action(gamedata.ply, model, simVar)\n",
    "        return bestmove, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de584816-f491-4f59-a8e0-fed14b8fdd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcts_nn = NetworkMCTS()\n",
    "\n",
    "MCTSAI =  MCTS_AI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c60bc97f-46d4-4b91-903b-a3d4905db0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def self_play_game(model, moveMax, player1bot = True, player2bot = True):\n",
    "    board_data_B = []\n",
    "    board_data_W = []\n",
    "    values_B = []\n",
    "    values_W = []\n",
    "    moves_B = []\n",
    "    moves_W = []\n",
    "    game = QuantumChessGame()\n",
    "    game.new_game({'initial_state_fen':get_puzzle_fen(34),  'max_split_moves':[1,1]});\n",
    "    movecode = 0;\n",
    "    while not game.is_game_over():\n",
    "        gamedata = game.get_game_data()\n",
    "\n",
    "        board_data_W.append(gamedata)\n",
    "        values_W.append(0)\n",
    "        board_data_B.append(gamedata)\n",
    "        values_B.append(0)\n",
    "\n",
    "        #best_move, value = mcts_nn.find_best_move(game, model, 30)\n",
    "\n",
    "        #best_move = MCTSAI.find_best_move(game, 35)\n",
    "        \n",
    "        #print(f\"Value {value}\")\n",
    "        #print(\"found best move\")\n",
    "\n",
    "        \n",
    "        # Record the state, policy, and value\n",
    "        if (gamedata.ply % 2 == 0):\n",
    "            if(player1bot):\n",
    "                #best_move, value = MCTSAI.find_best_move(game,90)\n",
    "                #print(f\"Value {value}\")\n",
    "                best_move, value = mcts_nn.find_best_move(game, model, 25)\n",
    "            else:\n",
    "                best_move = input(\"Enter your move: \")\n",
    "                value = 0\n",
    "            board_data_W.append(gamedata)\n",
    "            values_W.append(value)\n",
    "\n",
    "        if (gamedata.ply % 2 == 1):\n",
    "            if(player2bot):\n",
    "                #best_move, value = MCTSAI.find_best_move(game, 90)\n",
    "                #print(f\"Value {value}\")\n",
    "                best_move, value = mcts_nn.find_best_move(game, model, 25)\n",
    "            else:\n",
    "                best_move = input(\"Enter your move: \")\n",
    "                value = 0\n",
    "            board_data_B.append(gamedata)\n",
    "            values_B.append(value)\n",
    "        \n",
    "        \n",
    "\n",
    "        if (gamedata.ply == moveMax):\n",
    "            return board_data_W, board_data_B, moves_W, moves_B, values_W, values_B, 0\n",
    "        \n",
    "        print(f\"player # {gamedata.ply}\")\n",
    "        print(f\"move taken {best_move}\")\n",
    "        \n",
    "        # Apply the move to the board\n",
    "        board_state, movecode = game.do_move(best_move)\n",
    "        move = game.get_unformatted_last_move()\n",
    "        print(f\"move {move}\")\n",
    "        if (gamedata.ply % 2 == 1):\n",
    "            moves_W.append(move)\n",
    "        if (gamedata.ply % 2 == 0):\n",
    "            moves_B.append(move)\n",
    "            \n",
    "        game.print_board_and_probabilities()\n",
    "        \n",
    "    if(movecode == 2):\n",
    "        return board_data_W, board_data_B, moves_W, moves_B, values_W, values_B, 1  # Return +1 for  white win, 0 for draw, -1 for black win\n",
    "\n",
    "    if(movecode == 3):\n",
    "        return board_data_W, board_data_B, moves_W, moves_B, values_W, values_B, 0  # Return +1 for  white win, 0 for draw, -1 for black win\n",
    "\n",
    "    if(movecode == 5):\n",
    "        return board_data_W, board_data_B, moves_W, moves_B, values_W, values_B, -1 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97f1c985-06ca-46f7-8c39-d94b183dd3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting epoch 1\n",
      "1 player # in model\n",
      "player # 1\n",
      "move taken a8^a7b8\n",
      "move {'piece': 14, 'square1': 56, 'square2': 48, 'square3': 57, 'type': 4, 'variant': 1, 'does_measurement': False, 'measurement_outcome': 1, 'promotion_piece': 0}\n",
      " +-------------------------------------------------+\n",
      "8|   .    50:k   .     .     .     .     .     .   |\n",
      "7|  50:k 100:P   .     .     .     .     .     .   |\n",
      "6| 100:P   .   100:N   .     .     .     .     .   |\n",
      "5|   .     .     .     .     .     .     .     .   |\n",
      "4|   .     .     .     .     .     .     .     .   |\n",
      "3|   .     .     .     .     .     .     .     .   |\n",
      "2|   .     .     .     .     .     .     .   100:K |\n",
      "1|   .     .     .     .     .     .     .     .   |\n",
      " +-------------------------------------------------+\n",
      "     a     b     c     d     e     f     g     h\n",
      "2 player # in model\n",
      "hit time limit\n",
      "player # 2\n",
      "move taken c6b8.m1\n",
      "move {'piece': 2, 'square1': 42, 'square2': 57, 'square3': 64, 'type': 2, 'variant': 3, 'does_measurement': True, 'measurement_outcome': 1, 'promotion_piece': 0}\n",
      " +-------------------------------------------------+\n",
      "8|   .   100:N   .     .     .     .     .     .   |\n",
      "7|  50:k 100:P   .     .     .     .     .     .   |\n",
      "6| 100:P   .     .     .     .     .     .     .   |\n",
      "5|   .     .     .     .     .     .     .     .   |\n",
      "4|   .     .     .     .     .     .     .     .   |\n",
      "3|   .     .     .     .     .     .     .     .   |\n",
      "2|   .     .     .     .     .     .     .   100:K |\n",
      "1|   .     .     .     .     .     .     .     .   |\n",
      " +-------------------------------------------------+\n",
      "     a     b     c     d     e     f     g     h\n",
      "3 player # in model\n",
      "player # 3\n",
      "move taken a7b8.m1\n",
      "move {'piece': 14, 'square1': 48, 'square2': 57, 'square3': 64, 'type': 2, 'variant': 3, 'does_measurement': True, 'measurement_outcome': 1, 'promotion_piece': 0}\n",
      " +-------------------------------------------------+\n",
      "8|   .   100:k   .     .     .     .     .     .   |\n",
      "7|   .   100:P   .     .     .     .     .     .   |\n",
      "6| 100:P   .     .     .     .     .     .     .   |\n",
      "5|   .     .     .     .     .     .     .     .   |\n",
      "4|   .     .     .     .     .     .     .     .   |\n",
      "3|   .     .     .     .     .     .     .     .   |\n",
      "2|   .     .     .     .     .     .     .   100:K |\n",
      "1|   .     .     .     .     .     .     .     .   |\n",
      " +-------------------------------------------------+\n",
      "     a     b     c     d     e     f     g     h\n",
      "4 player # in model\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 111\u001b[39m\n\u001b[32m    108\u001b[39m         model_scripted.save(\u001b[33m'\u001b[39m\u001b[33mpuzzle34.pt\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;66;03m# Save\u001b[39;00m\n\u001b[32m    109\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining finished\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mNNmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(NNmodel, player1bot, player2bot, games_per_epoch, epochs)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mstarting epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m game \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(games_per_epoch):\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     board_data_w, board_data_b, moves_W, moves_B, values_W, values_B, result = \u001b[43mself_play_game\u001b[49m\u001b[43m(\u001b[49m\u001b[43mNNmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplayer1bot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplayer2bot\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Play a game\u001b[39;00m\n\u001b[32m     11\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mgame \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgame\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m finished\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# Train the model on the collected game data\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mself_play_game\u001b[39m\u001b[34m(model, moveMax, player1bot, player2bot)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (gamedata.ply % \u001b[32m2\u001b[39m == \u001b[32m0\u001b[39m):\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m(player1bot):\n\u001b[32m     30\u001b[39m         \u001b[38;5;66;03m#best_move, value = MCTSAI.find_best_move(game,90)\u001b[39;00m\n\u001b[32m     31\u001b[39m         \u001b[38;5;66;03m#print(f\"Value {value}\")\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m         best_move, value = \u001b[43mmcts_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfind_best_move\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgame\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m25\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     34\u001b[39m         best_move = \u001b[38;5;28minput\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEnter your move: \u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mNetworkMCTS.find_best_move\u001b[39m\u001b[34m(self, game, model, simVar)\u001b[39m\n\u001b[32m      7\u001b[39m root = MCTS_NN.MCTS_Node(game, model)\n\u001b[32m      8\u001b[39m gamedata = game.get_game_data()\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m bestmove, value = \u001b[43mroot\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbest_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgamedata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msimVar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m bestmove, value\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Code\\Quantum-Chess-AI-Development\\MCTS_NN.py:173\u001b[39m, in \u001b[36mMCTS_Node.best_action\u001b[39m\u001b[34m(self, player, model, simVar, timeout)\u001b[39m\n\u001b[32m    170\u001b[39m simulation_no = simVar*l \u001b[38;5;66;03m#adaptive simulations\u001b[39;00m\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(simulation_no):\n\u001b[32m--> \u001b[39m\u001b[32m173\u001b[39m     v = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tree_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    175\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i == \u001b[32m0\u001b[39m:\n\u001b[32m    176\u001b[39m         v.tic1()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Code\\Quantum-Chess-AI-Development\\MCTS_NN.py:152\u001b[39m, in \u001b[36mMCTS_Node._tree_policy\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    150\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m current_node.expand()\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m         current_node = \u001b[43mcurrent_node\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbest_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[38;5;66;03m#print(f\"Terminal node reached: {current_node.parent_action}\")\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m current_node\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Code\\Quantum-Chess-AI-Development\\MCTS_NN.py:129\u001b[39m, in \u001b[36mMCTS_Node.best_child\u001b[39m\u001b[34m(self, c_param)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbest_child\u001b[39m(\u001b[38;5;28mself\u001b[39m, c_param=\u001b[32m0.4\u001b[39m):\n\u001b[32m    128\u001b[39m     \u001b[38;5;66;03m#classic MCTS equation, c_param could probably be tweaked a bit\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m     choices_weights = [(np.multiply(\u001b[43mc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn_rating\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, \u001b[32m0.4\u001b[39m)) + (c.q() / c.n()) + c_param * np.sqrt((\u001b[32m2\u001b[39m * np.log(\u001b[38;5;28mself\u001b[39m.n()) / c.n())) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children] \n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children[np.argmax(choices_weights)]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Code\\Quantum-Chess-AI-Development\\MCTS_NN.py:64\u001b[39m, in \u001b[36mMCTS_Node.nn_rating\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     61\u001b[39m tensor = torch.zeros(\u001b[32m1\u001b[39m,\u001b[32m12\u001b[39m,\u001b[32m8\u001b[39m,\u001b[32m8\u001b[39m)\n\u001b[32m     62\u001b[39m tensor[\u001b[32m0\u001b[39m] = gameToTensor(\u001b[38;5;28mself\u001b[39m.game_state.get_game_data(), \u001b[38;5;28mself\u001b[39m.game_state.get_game_data().ply)\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m NNrating = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mNNmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m].item()\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m NNrating,\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Code\\Quantum-Chess-AI-Development\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Code\\Quantum-Chess-AI-Development\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Training loop\n",
    "def train_model(NNmodel, player1bot, player2bot, games_per_epoch, epochs):\n",
    "    optimizer = torch.optim.Adam(NNmodel.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    game_tensor = torch.zeros(1,12,8,8)\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"starting epoch {epoch + 1}\")\n",
    "        for game in range(games_per_epoch):\n",
    "            \n",
    "            \n",
    "            board_data_w, board_data_b, moves_W, moves_B, values_W, values_B, result = self_play_game(NNmodel, 9, player1bot, player2bot)  # Play a game\n",
    "            print(f\"game {game + 1} finished\")\n",
    "            # Train the model on the collected game data\n",
    "            \n",
    "            for i in range(len(moves_B)):\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                game_tensor[0] = gameToTensor(board_data_b[i], 1)\n",
    "\n",
    "                predicted_value = NNmodel(game_tensor)\n",
    "                true_value = torch.tensor([[values_B[i]]], dtype=torch.float32)\n",
    "\n",
    "                #print(f\"predicted value: \", {type(predicted_value[0])}, \"true value: \", {type(true_value)})\n",
    "                value_loss = F.mse_loss(predicted_value[0], true_value)\n",
    "                \n",
    "\n",
    "                #print(predicted_value[0])\n",
    "\n",
    "                piece = predicted_value[1][0]\n",
    "                pos1 = predicted_value[1][1]\n",
    "                pos2 = predicted_value[1][2]\n",
    "                pos3 = predicted_value[1][3]\n",
    "                move_type = predicted_value[1][4]\n",
    "                variation = predicted_value[1][5]\n",
    "                #print(piece)\n",
    "                #print(moves_B[i]['piece'])\n",
    "                # Assuming you have the true labels for the policy head\n",
    "                true_piece = torch.tensor([[round(moves_B[i]['piece'])]], dtype=torch.float)  # Replace with actual label\n",
    "                true_pos1 = torch.tensor([[round(moves_B[i]['square1'])]], dtype=torch.float)   # Replace with actual label\n",
    "                true_pos2 = torch.tensor([[round(moves_B[i]['square2'])]], dtype=torch.float)   # Replace with actual label\n",
    "                true_pos3 = torch.tensor([[round(moves_B[i]['square3'])]], dtype=torch.float)   # Replace with actual label\n",
    "                true_move_type = torch.tensor([[round(moves_B[i]['type'])]], dtype=torch.float)  # Replace with actual label\n",
    "                true_variation = torch.tensor([[round(moves_B[i]['variant'])]], dtype=torch.float)  # Replace with actual label\n",
    "\n",
    "                # Compute policy loss\n",
    "                policy_loss = (\n",
    "                    F.cross_entropy(piece, true_piece) +\n",
    "                    F.cross_entropy(pos1, true_pos1) +\n",
    "                    F.cross_entropy(pos2, true_pos2) +\n",
    "                    F.cross_entropy(pos3, true_pos3) +\n",
    "                    F.cross_entropy(move_type, true_move_type) +\n",
    "                    F.cross_entropy(variation, true_variation)\n",
    "                )\n",
    "                \n",
    "                # Combine value loss and policy loss\n",
    "                total_loss = value_loss + policy_loss\n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "            \n",
    "            for i in range(len(moves_B)):\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                game_tensor[0] = gameToTensor(board_data_w[i], 1)\n",
    "\n",
    "                predicted_value = NNmodel(game_tensor)\n",
    "                true_value = torch.tensor([[values_W[i]]], dtype=torch.float32)\n",
    "\n",
    "                #print(f\"predicted value: \", {type(predicted_value[0])}, \"true value: \", {type(true_value)})\n",
    "                value_loss = F.mse_loss(predicted_value[0], true_value)\n",
    "                \n",
    "\n",
    "                #print(predicted_value[0])\n",
    "\n",
    "                piece = predicted_value[1][0]\n",
    "                pos1 = predicted_value[1][1]\n",
    "                pos2 = predicted_value[1][2]\n",
    "                pos3 = predicted_value[1][3]\n",
    "                move_type = predicted_value[1][4]\n",
    "                variation = predicted_value[1][5]\n",
    "                #print(piece)\n",
    "                \n",
    "\n",
    "                # Using True values from the move made\n",
    "                true_piece = torch.tensor([[round(moves_W[i]['piece'])]], dtype=torch.float)  # Replace with actual label\n",
    "                true_pos1 = torch.tensor([[round(moves_W[i]['square1'])]], dtype=torch.float)   # Replace with actual label\n",
    "                true_pos2 = torch.tensor([[round(moves_W[i]['square2'])]], dtype=torch.float)   # Replace with actual label\n",
    "                true_pos3 = torch.tensor([[round(moves_W[i]['square3'])]], dtype=torch.float)   # Replace with actual label\n",
    "                true_move_type = torch.tensor([[round(moves_W[i]['type'])]], dtype=torch.float)  # Replace with actual label\n",
    "                true_variation = torch.tensor([[round(moves_W[i]['variant'])]], dtype=torch.float)  # Replace with actual label\n",
    "\n",
    "                # Compute policy loss\n",
    "                policy_loss = (\n",
    "                    F.cross_entropy(piece, true_piece) +\n",
    "                    F.cross_entropy(pos1, true_pos1) +\n",
    "                    F.cross_entropy(pos2, true_pos2) +\n",
    "                    F.cross_entropy(pos3, true_pos3) +\n",
    "                    F.cross_entropy(move_type, true_move_type) +\n",
    "                    F.cross_entropy(variation, true_variation)\n",
    "                )\n",
    "                \n",
    "                # Combine value loss and policy loss\n",
    "                total_loss = value_loss + policy_loss\n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        print(f\"epoch {epoch + 1} finished\")\n",
    "        model_scripted = torch.jit.script(NNmodel) # Export to TorchScript\n",
    "        model_scripted.save('puzzle34.pt') # Save\n",
    "    print(\"Training finished\")\n",
    "\n",
    "train_model(NNmodel, True, True, 3, 10)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31240686-57e0-4efb-a34b-22a4714ae2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "game = QuantumChessGame()\n",
    "game.new_game({'initial_state_fen':get_puzzle_fen(33),  'max_split_moves':[0,1]});\n",
    "game_tensor[0] = gameToTensor(game.get_game_data(), 0)\n",
    "\n",
    "output = NNmodel(game_tensor)\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
